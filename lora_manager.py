"""
FineTunedLLM LoRA Manager for Business Infinity

Manages LoRA (Low-Rank Adaptation) adapters for domain-specific LLM fine-tuning
using Azure Machine Learning. Provides legendary domain expertise to Business
Infinity agents through specialized LoRA adapters trained on legendary profiles.
"""

import os
import json
import asyncio
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from pathlib import Path

# Azure ML SDK imports
try:
    from azure.ai.ml import MLClient
    from azure.ai.ml.entities import Model, Environment
    from azure.identity import DefaultAzureCredential
    AZURE_ML_AVAILABLE = True
except ImportError:
    AZURE_ML_AVAILABLE = False
    logging.warning("Azure ML SDK not available")

# PyTorch and transformers for LoRA
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import LoraConfig, get_peft_model, TaskType
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False
    logging.warning("PyTorch/transformers not available")


@dataclass
class LegendaryProfile:
    """Represents a legendary business leader profile for LoRA training"""
    name: str
    domain: str
    expertise_areas: List[str]
    biographical_data: Dict[str, Any]
    decision_patterns: List[str]
    quotes_and_principles: List[str]
    case_studies: List[Dict[str, Any]]
    performance_metrics: Dict[str, float] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class LoRAAdapter:
    """Represents a LoRA adapter for legendary expertise"""
    adapter_id: str
    legendary_profile: str
    domain: str
    base_model: str
    adapter_path: str
    training_data_hash: str
    performance_score: float
    created_at: datetime
    last_used: Optional[datetime] = None
    usage_count: int = 0


class LoRAManager:
    """
    Manages LoRA adapters for legendary domain expertise in Business Infinity.
    
    Handles:
    - Loading and managing legendary profiles
    - Training LoRA adapters on Azure ML
    - Serving adapters for inference
    - Performance monitoring and optimization
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Azure ML configuration
        self.subscription_id = os.getenv("AZURE_SUBSCRIPTION_ID")\n        self.resource_group = os.getenv("AZURE_RESOURCE_GROUP", "business-infinity-rg")\n        self.workspace_name = os.getenv("AZURE_ML_WORKSPACE", "business-infinity-ml")\n        \n        # Model configuration\n        self.base_model = os.getenv("BASE_LLM_MODEL", "microsoft/DialoGPT-large")\n        self.max_sequence_length = int(os.getenv("MAX_SEQUENCE_LENGTH", "1024"))\n        \n        # Storage paths\n        self.adapters_path = Path(os.getenv("LORA_ADAPTERS_PATH", "models/lora_adapters"))\n        self.profiles_path = Path(os.getenv("LEGENDARY_PROFILES_PATH", "data/legendary_profiles"))\n        \n        # Core components\n        self.ml_client = None\n        self.legendary_profiles: Dict[str, LegendaryProfile] = {}\n        self.loaded_adapters: Dict[str, LoRAAdapter] = {}\n        self.base_tokenizer = None\n        self.base_model_instance = None\n        \n        # Performance tracking\n        self.adapter_performance: Dict[str, List[float]] = {}\n    \n    async def initialize(self):\n        \"\"\"Initialize the LoRA Manager\"\"\"\n        try:\n            self.logger.info("Initializing FineTunedLLM LoRA Manager...")\n            \n            # Initialize Azure ML client\n            if AZURE_ML_AVAILABLE and self.subscription_id:\n                credential = DefaultAzureCredential()\n                self.ml_client = MLClient(\n                    credential=credential,\n                    subscription_id=self.subscription_id,\n                    resource_group_name=self.resource_group,\n                    workspace_name=self.workspace_name\n                )\n                self.logger.info("Azure ML client initialized")\n            \n            # Load base model and tokenizer\n            if PYTORCH_AVAILABLE:\n                await self._load_base_model()\n            \n            # Load legendary profiles\n            await self._load_legendary_profiles()\n            \n            # Load existing adapters\n            await self._load_existing_adapters()\n            \n            # Create directories\n            self.adapters_path.mkdir(parents=True, exist_ok=True)\n            self.profiles_path.mkdir(parents=True, exist_ok=True)\n            \n            self.logger.info("LoRA Manager initialized successfully")\n            \n        except Exception as e:\n            self.logger.error(f"Failed to initialize LoRA Manager: {e}")\n            raise\n    \n    async def _load_base_model(self):\n        \"\"\"Load the base LLM model and tokenizer\"\"\"\n        try:\n            self.base_tokenizer = AutoTokenizer.from_pretrained(self.base_model)\n            if self.base_tokenizer.pad_token is None:\n                self.base_tokenizer.pad_token = self.base_tokenizer.eos_token\n            \n            self.base_model_instance = AutoModelForCausalLM.from_pretrained(\n                self.base_model,\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                device_map="auto" if torch.cuda.is_available() else None\n            )\n            \n            self.logger.info(f"Base model {self.base_model} loaded successfully")\n            \n        except Exception as e:\n            self.logger.error(f"Failed to load base model: {e}")\n            raise\n    \n    async def _load_legendary_profiles(self):\n        \"\"\"Load legendary business leader profiles\"\"\"\n        try:\n            # Load built-in legendary profiles\n            builtin_profiles = self._get_builtin_legendary_profiles()\n            \n            for profile in builtin_profiles:\n                self.legendary_profiles[profile.name] = profile\n            \n            # Load custom profiles from files\n            if self.profiles_path.exists():\n                for profile_file in self.profiles_path.glob("*.json"):\n                    try:\n                        with open(profile_file, 'r', encoding='utf-8') as f:\n                            profile_data = json.load(f)\n                        \n                        profile = LegendaryProfile(**profile_data)\n                        self.legendary_profiles[profile.name] = profile\n                        \n                    except Exception as e:\n                        self.logger.error(f"Failed to load profile {profile_file}: {e}")\n            \n            self.logger.info(f"Loaded {len(self.legendary_profiles)} legendary profiles")\n            \n        except Exception as e:\n            self.logger.error(f"Failed to load legendary profiles: {e}")\n    \n    def _get_builtin_legendary_profiles(self) -> List[LegendaryProfile]:\n        \"\"\"Get built-in legendary business leader profiles\"\"\"\n        return [\n            LegendaryProfile(\n                name="Warren Buffett",\n                domain="Investment Strategy",\n                expertise_areas=["value_investing", "market_analysis", "risk_assessment", "long_term_thinking"],\n                biographical_data={\n                    "era": "1930-present",\n                    "company": "Berkshire Hathaway",\n                    "known_for": "Value investing, long-term wealth creation",\n                    "investment_philosophy": "Buy wonderful companies at fair prices"\n                },\n                decision_patterns=[\n                    "Focus on intrinsic value over market price",\n                    "Invest in businesses you understand",\n                    "Think long-term, ignore short-term volatility",\n                    "Maintain discipline in valuation"\n                ],\n                quotes_and_principles=[\n                    "Be fearful when others are greedy and greedy when others are fearful",\n                    "Rule No. 1: Never lose money. Rule No. 2: Never forget rule No. 1",\n                    "Price is what you pay; value is what you get"\n                ],\n                case_studies=[\n                    {\n                        "situation": "2008 Financial Crisis",\n                        "decision": "Invested heavily in undervalued companies",\n                        "outcome": "Significant returns when markets recovered"\n                    }\n                ]\n            ),\n            LegendaryProfile(\n                name="Steve Jobs",\n                domain="Innovation & Vision",\n                expertise_areas=["product_vision", "innovation", "brand_building", "market_disruption"],\n                biographical_data={\n                    "era": "1955-2011",\n                    "company": "Apple Inc.",\n                    "known_for": "Revolutionary product design, market transformation",\n                    "philosophy": "Think different, focus on user experience"\n                },\n                decision_patterns=[\n                    "Prioritize simplicity and elegance",\n                    "Focus on user experience over features",\n                    "Create products customers don't know they need",\n                    "Maintain perfectionist standards"\n                ],\n                quotes_and_principles=[\n                    "Innovation distinguishes between a leader and a follower",\n                    "Stay hungry, stay foolish",\n                    "It's better to be a pirate than to join the navy"\n                ],\n                case_studies=[\n                    {\n                        "situation": "Return to Apple in 1997",\n                        "decision": "Simplified product line, focused on design",\n                        "outcome": "Transformed Apple into world's most valuable company"\n                    }\n                ]\n            ),\n            # Add more legendary profiles...\n        ]\n    \n    async def _load_existing_adapters(self):\n        \"\"\"Load existing LoRA adapters from storage\"\"\"\n        try:\n            adapter_registry_file = self.adapters_path / "adapter_registry.json"\n            \n            if adapter_registry_file.exists():\n                with open(adapter_registry_file, 'r', encoding='utf-8') as f:\n                    adapter_data = json.load(f)\n                \n                for adapter_id, adapter_info in adapter_data.items():\n                    adapter = LoRAAdapter(\n                        adapter_id=adapter_id,\n                        legendary_profile=adapter_info["legendary_profile"],\n                        domain=adapter_info["domain"],\n                        base_model=adapter_info["base_model"],\n                        adapter_path=adapter_info["adapter_path"],\n                        training_data_hash=adapter_info["training_data_hash"],\n                        performance_score=adapter_info["performance_score"],\n                        created_at=datetime.fromisoformat(adapter_info["created_at"]),\n                        last_used=datetime.fromisoformat(adapter_info["last_used"]) if adapter_info.get("last_used") else None,\n                        usage_count=adapter_info.get("usage_count", 0)\n                    )\n                    \n                    self.loaded_adapters[adapter_id] = adapter\n                \n                self.logger.info(f"Loaded {len(self.loaded_adapters)} existing adapters")\n            \n        except Exception as e:\n            self.logger.error(f"Failed to load existing adapters: {e}")\n    \n    async def load_legendary_adapter(self, legend_name: str, domain: str) -> str:\n        \"\"\"\n        Load or create a LoRA adapter for legendary expertise\n        \n        Args:\n            legend_name: Name of the legendary figure\n            domain: Domain of expertise\n            \n        Returns:\n            Adapter ID for the loaded/created adapter\n        \"\"\"\n        try:\n            # Check if adapter already exists\n            adapter_id = f"{legend_name.lower().replace(' ', '_')}_{domain.lower().replace(' ', '_')}\"\n            \n            if adapter_id in self.loaded_adapters:\n                adapter = self.loaded_adapters[adapter_id]\n                adapter.last_used = datetime.now()\n                adapter.usage_count += 1\n                self.logger.info(f"Using existing adapter: {adapter_id}\")\n                return adapter_id\n            \n            # Check if legendary profile exists\n            if legend_name not in self.legendary_profiles:\n                raise ValueError(f\"Legendary profile '{legend_name}' not found\")\n            \n            # Create new adapter if it doesn't exist\n            adapter_id = await self._create_legendary_adapter(legend_name, domain)\n            \n            return adapter_id\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to load legendary adapter for {legend_name}: {e}\")\n            return f\"fallback_{legend_name.lower().replace(' ', '_')}\"\n    \n    async def _create_legendary_adapter(self, legend_name: str, domain: str) -> str:\n        \"\"\"Create a new LoRA adapter for legendary expertise\"\"\"\n        try:\n            profile = self.legendary_profiles[legend_name]\n            adapter_id = f\"{legend_name.lower().replace(' ', '_')}_{domain.lower().replace(' ', '_')}\"\n            \n            # Generate training data from legendary profile\n            training_data = self._generate_training_data(profile)\n            \n            # Create LoRA configuration\n            lora_config = LoraConfig(\n                task_type=TaskType.CAUSAL_LM,\n                inference_mode=False,\n                r=16,  # Rank\n                lora_alpha=32,\n                lora_dropout=0.1,\n                target_modules=[\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"]\n            )\n            \n            # Train the adapter (simplified for demo)\n            adapter_path = self.adapters_path / adapter_id\n            adapter_path.mkdir(exist_ok=True)\n            \n            # In a real implementation, this would train the LoRA adapter\n            # For now, we'll create a placeholder\n            adapter_info = {\n                \"config\": lora_config.__dict__,\n                \"training_data_size\": len(training_data),\n                \"legend_name\": legend_name,\n                \"domain\": domain\n            }\n            \n            with open(adapter_path / \"adapter_config.json\", 'w') as f:\n                json.dump(adapter_info, f, indent=2)\n            \n            # Create adapter record\n            adapter = LoRAAdapter(\n                adapter_id=adapter_id,\n                legendary_profile=legend_name,\n                domain=domain,\n                base_model=self.base_model,\n                adapter_path=str(adapter_path),\n                training_data_hash=self._hash_training_data(training_data),\n                performance_score=0.85,  # Would be calculated from training\n                created_at=datetime.now()\n            )\n            \n            self.loaded_adapters[adapter_id] = adapter\n            \n            # Save adapter registry\n            await self._save_adapter_registry()\n            \n            self.logger.info(f\"Created new LoRA adapter: {adapter_id}\")\n            return adapter_id\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to create legendary adapter: {e}\")\n            raise\n    \n    def _generate_training_data(self, profile: LegendaryProfile) -> List[Dict[str, str]]:\n        \"\"\"Generate training data from legendary profile\"\"\"\n        training_data = []\n        \n        # Convert decision patterns to training examples\n        for pattern in profile.decision_patterns:\n            training_data.append({\n                \"input\": f\"How would {profile.name} approach this situation?\",\n                \"output\": f\"Based on {profile.name}'s approach: {pattern}\"\n            })\n        \n        # Convert quotes to training examples\n        for quote in profile.quotes_and_principles:\n            training_data.append({\n                \"input\": f\"What is {profile.name}'s philosophy?\",\n                \"output\": f\"{profile.name} believes: {quote}\"\n            })\n        \n        # Convert case studies to training examples\n        for case in profile.case_studies:\n            training_data.append({\n                \"input\": f\"In {case['situation']}, what would {profile.name} do?\",\n                \"output\": f\"{profile.name} would: {case['decision']}. This approach typically results in: {case['outcome']}\"\n            })\n        \n        return training_data\n    \n    def _hash_training_data(self, training_data: List[Dict[str, str]]) -> str:\n        \"\"\"Generate hash for training data\"\"\"\n        import hashlib\n        data_str = json.dumps(training_data, sort_keys=True)\n        return hashlib.sha256(data_str.encode()).hexdigest()[:16]\n    \n    async def _save_adapter_registry(self):\n        \"\"\"Save adapter registry to storage\"\"\"\n        try:\n            registry_data = {}\n            \n            for adapter_id, adapter in self.loaded_adapters.items():\n                registry_data[adapter_id] = {\n                    \"legendary_profile\": adapter.legendary_profile,\n                    \"domain\": adapter.domain,\n                    \"base_model\": adapter.base_model,\n                    \"adapter_path\": adapter.adapter_path,\n                    \"training_data_hash\": adapter.training_data_hash,\n                    \"performance_score\": adapter.performance_score,\n                    \"created_at\": adapter.created_at.isoformat(),\n                    \"last_used\": adapter.last_used.isoformat() if adapter.last_used else None,\n                    \"usage_count\": adapter.usage_count\n                }\n            \n            registry_file = self.adapters_path / \"adapter_registry.json\"\n            with open(registry_file, 'w', encoding='utf-8') as f:\n                json.dump(registry_data, f, indent=2)\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to save adapter registry: {e}\")\n    \n    async def get_legendary_response(self, adapter_id: str, prompt: str, context: Dict[str, Any] = None) -> str:\n        \"\"\"\n        Generate response using legendary expertise adapter\n        \n        Args:\n            adapter_id: ID of the LoRA adapter to use\n            prompt: Input prompt for the legendary agent\n            context: Additional context for the response\n            \n        Returns:\n            Generated response from legendary perspective\n        \"\"\"\n        try:\n            if adapter_id not in self.loaded_adapters:\n                return f\"Adapter {adapter_id} not found. Using fallback response.\"\n            \n            adapter = self.loaded_adapters[adapter_id]\n            \n            # Update usage tracking\n            adapter.last_used = datetime.now()\n            adapter.usage_count += 1\n            \n            # For demo, return a legendary-style response\n            # In production, this would use the actual LoRA adapter\n            legendary_context = f\"From {adapter.legendary_profile}'s {adapter.domain} perspective:\"\n            \n            return f\"{legendary_context} {prompt} - Based on legendary expertise and proven patterns...\"\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to generate legendary response: {e}\")\n            return \"Error generating legendary response\"\n    \n    async def get_adapter_performance(self, adapter_id: str) -> Dict[str, Any]:\n        \"\"\"Get performance metrics for a specific adapter\"\"\"\n        if adapter_id not in self.loaded_adapters:\n            return {\"status\": \"not_found\"}\n        \n        adapter = self.loaded_adapters[adapter_id]\n        \n        return {\n            \"adapter_id\": adapter_id,\n            \"legendary_profile\": adapter.legendary_profile,\n            \"domain\": adapter.domain,\n            \"performance_score\": adapter.performance_score,\n            \"usage_count\": adapter.usage_count,\n            \"last_used\": adapter.last_used.isoformat() if adapter.last_used else None,\n            \"created_at\": adapter.created_at.isoformat()\n        }\n    \n    async def list_available_adapters(self) -> List[Dict[str, Any]]:\n        \"\"\"List all available LoRA adapters\"\"\"\n        adapters = []\n        \n        for adapter_id, adapter in self.loaded_adapters.items():\n            adapters.append({\n                \"adapter_id\": adapter_id,\n                \"legendary_profile\": adapter.legendary_profile,\n                \"domain\": adapter.domain,\n                \"performance_score\": adapter.performance_score,\n                \"usage_count\": adapter.usage_count,\n                \"created_at\": adapter.created_at.isoformat()\n            })\n        \n        return sorted(adapters, key=lambda x: x[\"usage_count\"], reverse=True)\n    \n    async def list_legendary_profiles(self) -> List[Dict[str, Any]]:\n        \"\"\"List all available legendary profiles\"\"\"\n        profiles = []\n        \n        for name, profile in self.legendary_profiles.items():\n            profiles.append({\n                \"name\": profile.name,\n                \"domain\": profile.domain,\n                \"expertise_areas\": profile.expertise_areas,\n                \"available_adapters\": [\n                    adapter_id for adapter_id, adapter in self.loaded_adapters.items()\n                    if adapter.legendary_profile == name\n                ]\n            })\n        \n        return profiles\n    \n    async def shutdown(self):\n        \"\"\"Graceful shutdown of LoRA Manager\"\"\"\n        try:\n            # Save current state\n            await self._save_adapter_registry()\n            \n            # Clear loaded models to free memory\n            if self.base_model_instance:\n                del self.base_model_instance\n                self.base_model_instance = None\n            \n            if self.base_tokenizer:\n                del self.base_tokenizer\n                self.base_tokenizer = None\n            \n            self.logger.info(\"LoRA Manager shutdown completed\")\n            \n        except Exception as e:\n            self.logger.error(f\"Error during LoRA Manager shutdown: {e}\")