"""
Mentor Mode Training System for FineTunedLLM

Provides interactive training and refinement capabilities for LoRA adapters
based on user feedback, performance metrics, and continuous learning from
business outcomes. Enables domain experts to fine-tune legendary AI agents.
"""

import os
import json
import asyncio
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from pathlib import Path
from enum import Enum
import numpy as np


class TrainingPhase(Enum):
    """Training phases for mentor mode"""
    INITIAL = "initial"
    FEEDBACK = "feedback"
    REFINEMENT = "refinement"
    VALIDATION = "validation"
    DEPLOYMENT = "deployment"


class FeedbackType(Enum):
    """Types of feedback for training"""
    POSITIVE = "positive"
    NEGATIVE = "negative"
    CORRECTION = "correction"
    ENHANCEMENT = "enhancement"


@dataclass
class TrainingFeedback:
    """Feedback record for training sessions"""
    feedback_id: str
    session_id: str
    adapter_id: str
    prompt: str
    original_response: str
    feedback_type: FeedbackType
    feedback_text: str
    corrected_response: Optional[str] = None
    rating: Optional[float] = None
    user_id: str = "system"
    timestamp: datetime = field(default_factory=datetime.now)
    applied: bool = False


@dataclass
class TrainingSession:
    """Training session for a specific adapter"""
    session_id: str
    adapter_id: str
    legendary_profile: str
    domain: str
    phase: TrainingPhase
    start_time: datetime
    end_time: Optional[datetime] = None
    feedback_count: int = 0
    improvement_score: float = 0.0
    training_examples: List[Dict[str, Any]] = field(default_factory=list)
    feedback_records: List[TrainingFeedback] = field(default_factory=list)
    performance_metrics: Dict[str, float] = field(default_factory=dict)
    status: str = "active"


class MentorMode:
    """
    Mentor Mode Training System for Business Infinity legendary agents.
    
    Provides:
    - Interactive training sessions
    - Feedback collection and processing
    - Continuous learning from business outcomes
    - Performance monitoring and optimization
    - Domain expert collaboration
    """
    
    def __init__(self, lora_manager):
        self.lora_manager = lora_manager
        self.logger = logging.getLogger(__name__)
        
        # Training configuration
        self.training_data_path = Path(os.getenv("MENTOR_TRAINING_PATH", "data/mentor_training"))
        self.session_timeout = timedelta(hours=int(os.getenv("MENTOR_SESSION_TIMEOUT", "4")))
        self.min_feedback_count = int(os.getenv("MIN_FEEDBACK_COUNT", "10"))
        
        # Active sessions and feedback
        self.active_sessions: Dict[str, TrainingSession] = {}
        self.feedback_queue: List[TrainingFeedback] = []
        self.performance_history: Dict[str, List[Dict[str, Any]]] = {}
        
        # Training metrics
        self.training_metrics = {
            "total_sessions": 0,
            "total_feedback": 0,
            "average_improvement": 0.0,
            "successful_deployments": 0
        }
    
    async def initialize(self):
        """Initialize Mentor Mode training system"""
        try:\n            self.logger.info("Initializing Mentor Mode training system...")\n            \n            # Create training directories\n            self.training_data_path.mkdir(parents=True, exist_ok=True)\n            (self.training_data_path / "sessions").mkdir(exist_ok=True)\n            (self.training_data_path / "feedback").mkdir(exist_ok=True)\n            (self.training_data_path / "models").mkdir(exist_ok=True)\n            \n            # Load existing sessions\n            await self._load_existing_sessions()\n            \n            # Load performance history\n            await self._load_performance_history()\n            \n            # Load training metrics\n            await self._load_training_metrics()\n            \n            self.logger.info("Mentor Mode initialized successfully")\n            \n        except Exception as e:\n            self.logger.error(f"Failed to initialize Mentor Mode: {e}")\n            raise\n    \n    async def _load_existing_sessions(self):\n        """Load existing training sessions"""\n        try:\n            sessions_dir = self.training_data_path / "sessions"\n            \n            for session_file in sessions_dir.glob("*.json"):\n                try:\n                    with open(session_file, 'r', encoding='utf-8') as f:\n                        session_data = json.load(f)\n                    \n                    # Convert feedback records\n                    feedback_records = []\n                    for fb_data in session_data.get("feedback_records", []):\n                        feedback = TrainingFeedback(\n                            feedback_id=fb_data["feedback_id"],\n                            session_id=fb_data["session_id"],\n                            adapter_id=fb_data["adapter_id"],\n                            prompt=fb_data["prompt"],\n                            original_response=fb_data["original_response"],\n                            feedback_type=FeedbackType(fb_data["feedback_type"]),\n                            feedback_text=fb_data["feedback_text"],\n                            corrected_response=fb_data.get("corrected_response"),\n                            rating=fb_data.get("rating"),\n                            user_id=fb_data.get("user_id", "system"),\n                            timestamp=datetime.fromisoformat(fb_data["timestamp"]),\n                            applied=fb_data.get("applied", False)\n                        )\n                        feedback_records.append(feedback)\n                    \n                    # Create session object\n                    session = TrainingSession(\n                        session_id=session_data["session_id"],\n                        adapter_id=session_data["adapter_id"],\n                        legendary_profile=session_data["legendary_profile"],\n                        domain=session_data["domain"],\n                        phase=TrainingPhase(session_data["phase"]),\n                        start_time=datetime.fromisoformat(session_data["start_time"]),\n                        end_time=datetime.fromisoformat(session_data["end_time"]) if session_data.get("end_time") else None,\n                        feedback_count=session_data.get("feedback_count", 0),\n                        improvement_score=session_data.get("improvement_score", 0.0),\n                        training_examples=session_data.get("training_examples", []),\n                        feedback_records=feedback_records,\n                        performance_metrics=session_data.get("performance_metrics", {}),\n                        status=session_data.get("status", "active")\n                    )\n                    \n                    if session.status == "active":\n                        self.active_sessions[session.session_id] = session\n                    \n                except Exception as e:\n                    self.logger.error(f"Failed to load session {session_file}: {e}")\n            \n            self.logger.info(f"Loaded {len(self.active_sessions)} active training sessions")\n            \n        except Exception as e:\n            self.logger.error(f"Failed to load existing sessions: {e}")\n    \n    async def _load_performance_history(self):\n        """Load performance history"""\n        try:\n            history_file = self.training_data_path / "performance_history.json"\n            \n            if history_file.exists():\n                with open(history_file, 'r', encoding='utf-8') as f:\n                    self.performance_history = json.load(f)\n            \n            self.logger.info(f"Loaded performance history for {len(self.performance_history)} adapters")\n            \n        except Exception as e:\n            self.logger.error(f"Failed to load performance history: {e}")\n    \n    async def _load_training_metrics(self):\n        """Load training metrics"""\n        try:\n            metrics_file = self.training_data_path / "training_metrics.json"\n            \n            if metrics_file.exists():\n                with open(metrics_file, 'r', encoding='utf-8') as f:\n                    self.training_metrics.update(json.load(f))\n            \n        except Exception as e:\n            self.logger.error(f"Failed to load training metrics: {e}")\n    \n    async def start_training_session(self, \n                                   adapter_id: str, \n                                   domain_expert_id: str = "system",\n                                   training_goals: List[str] = None) -> str:\n        \"\"\"\n        Start a new training session for an adapter\n        \n        Args:\n            adapter_id: ID of the LoRA adapter to train\n            domain_expert_id: ID of the domain expert conducting training\n            training_goals: Specific goals for this training session\n            \n        Returns:\n            Session ID for the training session\n        \"\"\"\n        try:\n            # Verify adapter exists\n            adapter_info = await self.lora_manager.get_adapter_performance(adapter_id)\n            if adapter_info.get("status") == "not_found":\n                raise ValueError(f"Adapter {adapter_id} not found")\n            \n            # Generate session ID\n            session_id = f"mentor_{adapter_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n            \n            # Create training session\n            session = TrainingSession(\n                session_id=session_id,\n                adapter_id=adapter_id,\n                legendary_profile=adapter_info["legendary_profile"],\n                domain=adapter_info["domain"],\n                phase=TrainingPhase.INITIAL,\n                start_time=datetime.now()\n            )\n            \n            # Add training goals if provided\n            if training_goals:\n                session.training_examples.append({\n                    "type": "training_goals",\n                    "goals": training_goals,\n                    "timestamp": datetime.now().isoformat()\n                })\n            \n            self.active_sessions[session_id] = session\n            self.training_metrics["total_sessions"] += 1\n            \n            # Save session\n            await self._save_session(session)\n            \n            self.logger.info(f"Started training session: {session_id} for adapter: {adapter_id}")\n            return session_id\n            \n        except Exception as e:\n            self.logger.error(f"Failed to start training session: {e}")\n            raise\n    \n    async def provide_feedback(self, \n                             session_id: str,\n                             prompt: str,\n                             original_response: str,\n                             feedback_type: FeedbackType,\n                             feedback_text: str,\n                             corrected_response: Optional[str] = None,\n                             rating: Optional[float] = None,\n                             user_id: str = "system") -> str:\n        \"\"\"\n        Provide feedback for a training session\n        \n        Args:\n            session_id: ID of the training session\n            prompt: Original prompt that generated the response\n            original_response: Response that needs feedback\n            feedback_type: Type of feedback being provided\n            feedback_text: Detailed feedback text\n            corrected_response: Corrected version of the response (if applicable)\n            rating: Numerical rating (1-5 scale)\n            user_id: ID of the user providing feedback\n            \n        Returns:\n            Feedback ID\n        \"\"\"\n        try:\n            if session_id not in self.active_sessions:\n                raise ValueError(f"Training session {session_id} not found or inactive")\n            \n            session = self.active_sessions[session_id]\n            \n            # Generate feedback ID\n            feedback_id = f"fb_{session_id}_{len(session.feedback_records) + 1:03d}\"\n            \n            # Create feedback record\n            feedback = TrainingFeedback(\n                feedback_id=feedback_id,\n                session_id=session_id,\n                adapter_id=session.adapter_id,\n                prompt=prompt,\n                original_response=original_response,\n                feedback_type=feedback_type,\n                feedback_text=feedback_text,\n                corrected_response=corrected_response,\n                rating=rating,\n                user_id=user_id\n            )\n            \n            # Add to session and queue\n            session.feedback_records.append(feedback)\n            session.feedback_count += 1\n            self.feedback_queue.append(feedback)\n            \n            # Update training metrics\n            self.training_metrics["total_feedback"] += 1\n            \n            # Check if ready for next phase\n            await self._check_phase_transition(session)\n            \n            # Save session\n            await self._save_session(session)\n            \n            self.logger.info(f"Added feedback {feedback_id} to session {session_id}")\n            return feedback_id\n            \n        except Exception as e:\n            self.logger.error(f"Failed to provide feedback: {e}")\n            raise\n    \n    async def _check_phase_transition(self, session: TrainingSession):\n        """Check if session is ready to transition to next phase"""        try:\n            if session.phase == TrainingPhase.INITIAL and session.feedback_count >= 5:\n                session.phase = TrainingPhase.FEEDBACK\n                self.logger.info(f"Session {session.session_id} transitioned to FEEDBACK phase")\n            \n            elif session.phase == TrainingPhase.FEEDBACK and session.feedback_count >= self.min_feedback_count:\n                # Calculate improvement score\n                improvement_score = await self._calculate_improvement_score(session)\n                session.improvement_score = improvement_score\n                \n                if improvement_score >= 0.7:\n                    session.phase = TrainingPhase.VALIDATION\n                    self.logger.info(f"Session {session.session_id} transitioned to VALIDATION phase")\n                else:\n                    session.phase = TrainingPhase.REFINEMENT\n                    self.logger.info(f"Session {session.session_id} transitioned to REFINEMENT phase")\n            \n            elif session.phase == TrainingPhase.REFINEMENT and session.improvement_score >= 0.8:\n                session.phase = TrainingPhase.VALIDATION\n                self.logger.info(f"Session {session.session_id} transitioned to VALIDATION phase")\n            \n        except Exception as e:\n            self.logger.error(f"Failed to check phase transition: {e}")\n    \n    async def _calculate_improvement_score(self, session: TrainingSession) -> float:\n        \"\"\"Calculate improvement score based on feedback\"\"\"\n        try:\n            if not session.feedback_records:\n                return 0.0\n            \n            positive_weight = 1.0\n            negative_weight = -0.5\n            correction_weight = 0.8\n            enhancement_weight = 0.6\n            \n            total_score = 0.0\n            total_weight = 0.0\n            \n            for feedback in session.feedback_records:\n                weight = 0.0\n                score = 0.0\n                \n                if feedback.feedback_type == FeedbackType.POSITIVE:\n                    weight = positive_weight\n                    score = 1.0\n                elif feedback.feedback_type == FeedbackType.NEGATIVE:\n                    weight = abs(negative_weight)\n                    score = 0.0\n                elif feedback.feedback_type == FeedbackType.CORRECTION:\n                    weight = correction_weight\n                    score = 0.7\n                elif feedback.feedback_type == FeedbackType.ENHANCEMENT:\n                    weight = enhancement_weight\n                    score = 0.8\n                \n                # Factor in rating if provided\n                if feedback.rating is not None:\n                    score = (score + (feedback.rating / 5.0)) / 2.0\n                \n                total_score += score * weight\n                total_weight += weight\n            \n            return total_score / total_weight if total_weight > 0 else 0.0\n            \n        except Exception as e:\n            self.logger.error(f"Failed to calculate improvement score: {e}")\n            return 0.0\n    \n    async def get_training_suggestions(self, session_id: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Get training suggestions based on feedback patterns\n        \n        Args:\n            session_id: ID of the training session\n            \n        Returns:\n            List of training suggestions\n        \"\"\"\n        try:\n            if session_id not in self.active_sessions:\n                return []\n            \n            session = self.active_sessions[session_id]\n            suggestions = []\n            \n            # Analyze feedback patterns\n            feedback_analysis = self._analyze_feedback_patterns(session.feedback_records)\n            \n            # Generate suggestions based on analysis\n            if feedback_analysis[\"negative_count\"] > feedback_analysis[\"positive_count\"]:\n                suggestions.append({\n                    "type": "focus_area",\n                    "priority": "high",\n                    "suggestion": "Focus on addressing recurring negative feedback patterns",\n                    "details": feedback_analysis["common_issues"]\n                })\n            \n            if feedback_analysis[\"correction_count\"] > 3:\n                suggestions.append({\n                    "type": "training_data",\n                    "priority": "medium",\n                    "suggestion": "Consider expanding training data with corrected examples",\n                    "details": "Multiple corrections suggest gaps in training data"\n                })\n            \n            if len(feedback_analysis["enhancement_areas"]) > 0:\n                suggestions.append({\n                    "type": "enhancement",\n                    "priority": "low",\n                    "suggestion": "Explore enhancement opportunities",\n                    "details": feedback_analysis["enhancement_areas"]\n                })\n            \n            return suggestions\n            \n        except Exception as e:\n            self.logger.error(f"Failed to get training suggestions: {e}")\n            return []\n    \n    def _analyze_feedback_patterns(self, feedback_records: List[TrainingFeedback]) -> Dict[str, Any]:\n        \"\"\"Analyze feedback patterns to identify trends\"\"\"\n        analysis = {\n            \"positive_count\": 0,\n            \"negative_count\": 0,\n            \"correction_count\": 0,\n            \"enhancement_count\": 0,\n            \"common_issues\": [],\n            \"enhancement_areas\": [],\n            \"average_rating\": 0.0\n        }\n        \n        ratings = []\n        issue_keywords = {}\n        enhancement_keywords = {}\n        \n        for feedback in feedback_records:\n            # Count feedback types\n            if feedback.feedback_type == FeedbackType.POSITIVE:\n                analysis[\"positive_count\"] += 1\n            elif feedback.feedback_type == FeedbackType.NEGATIVE:\n                analysis[\"negative_count\"] += 1\n                # Extract issue keywords\n                words = feedback.feedback_text.lower().split()\n                for word in words:\n                    if len(word) > 4:  # Filter meaningful words\n                        issue_keywords[word] = issue_keywords.get(word, 0) + 1\n            elif feedback.feedback_type == FeedbackType.CORRECTION:\n                analysis[\"correction_count\"] += 1\n            elif feedback.feedback_type == FeedbackType.ENHANCEMENT:\n                analysis[\"enhancement_count\"] += 1\n                # Extract enhancement keywords\n                words = feedback.feedback_text.lower().split()\n                for word in words:\n                    if len(word) > 4:\n                        enhancement_keywords[word] = enhancement_keywords.get(word, 0) + 1\n            \n            # Collect ratings\n            if feedback.rating is not None:\n                ratings.append(feedback.rating)\n        \n        # Identify common issues and enhancements\n        analysis[\"common_issues\"] = [word for word, count in \n                                   sorted(issue_keywords.items(), key=lambda x: x[1], reverse=True)[:5]]\n        analysis[\"enhancement_areas\"] = [word for word, count in \n                                       sorted(enhancement_keywords.items(), key=lambda x: x[1], reverse=True)[:3]]\n        \n        # Calculate average rating\n        if ratings:\n            analysis[\"average_rating\"] = sum(ratings) / len(ratings)\n        \n        return analysis\n    \n    async def apply_feedback(self, session_id: str) -> bool:\n        \"\"\"\n        Apply collected feedback to improve the adapter\n        \n        Args:\n            session_id: ID of the training session\n            \n        Returns:\n            True if feedback was successfully applied\n        \"\"\"\n        try:\n            if session_id not in self.active_sessions:\n                raise ValueError(f"Training session {session_id} not found")\n            \n            session = self.active_sessions[session_id]\n            \n            # Check if session is ready for feedback application\n            if session.phase not in [TrainingPhase.REFINEMENT, TrainingPhase.VALIDATION]:\n                return False\n            \n            # Process feedback for training\n            training_examples = []\n            \n            for feedback in session.feedback_records:\n                if not feedback.applied:\n                    example = {\n                        \"input\": feedback.prompt,\n                        \"original_output\": feedback.original_response,\n                        \"feedback\": feedback.feedback_text,\n                        \"feedback_type\": feedback.feedback_type.value\n                    }\n                    \n                    if feedback.corrected_response:\n                        example[\"corrected_output\"] = feedback.corrected_response\n                    \n                    if feedback.rating:\n                        example[\"rating\"] = feedback.rating\n                    \n                    training_examples.append(example)\n                    feedback.applied = True\n            \n            # In a real implementation, this would update the LoRA adapter\n            # For now, we'll simulate the training process\n            self.logger.info(f\"Applying {len(training_examples)} feedback examples to adapter {session.adapter_id}\")\n            \n            # Update session metrics\n            session.improvement_score = await self._calculate_improvement_score(session)\n            \n            # Save updated session\n            await self._save_session(session)\n            \n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to apply feedback: {e}\")\n            return False\n    \n    async def complete_training_session(self, session_id: str) -> Dict[str, Any]:\n        \"\"\"\n        Complete a training session and deploy the improved adapter\n        \n        Args:\n            session_id: ID of the training session\n            \n        Returns:\n            Training completion summary\n        \"\"\"\n        try:\n            if session_id not in self.active_sessions:\n                raise ValueError(f"Training session {session_id} not found")\n            \n            session = self.active_sessions[session_id]\n            \n            # Final feedback application\n            await self.apply_feedback(session_id)\n            \n            # Update session status\n            session.phase = TrainingPhase.DEPLOYMENT\n            session.end_time = datetime.now()\n            session.status = "completed"\n            \n            # Update performance history\n            adapter_id = session.adapter_id\n            if adapter_id not in self.performance_history:\n                self.performance_history[adapter_id] = []\n            \n            self.performance_history[adapter_id].append({\n                \"session_id\": session_id,\n                \"improvement_score\": session.improvement_score,\n                \"feedback_count\": session.feedback_count,\n                \"completion_time\": session.end_time.isoformat(),\n                \"duration_hours\": (session.end_time - session.start_time).total_seconds() / 3600\n            })\n            \n            # Update metrics\n            self.training_metrics[\"successful_deployments\"] += 1\n            self.training_metrics[\"average_improvement\"] = (\n                (self.training_metrics[\"average_improvement\"] * (self.training_metrics[\"successful_deployments\"] - 1) +\n                 session.improvement_score) / self.training_metrics[\"successful_deployments\"]\n            )\n            \n            # Remove from active sessions\n            del self.active_sessions[session_id]\n            \n            # Save final session state\n            await self._save_session(session)\n            await self._save_performance_history()\n            await self._save_training_metrics()\n            \n            # Create completion summary\n            summary = {\n                \"session_id\": session_id,\n                \"adapter_id\": session.adapter_id,\n                \"legendary_profile\": session.legendary_profile,\n                \"domain\": session.domain,\n                \"start_time\": session.start_time.isoformat(),\n                \"end_time\": session.end_time.isoformat(),\n                \"duration_hours\": (session.end_time - session.start_time).total_seconds() / 3600,\n                \"feedback_count\": session.feedback_count,\n                \"improvement_score\": session.improvement_score,\n                \"final_performance\": session.performance_metrics,\n                \"status\": \"deployed\"\n            }\n            \n            self.logger.info(f\"Completed training session: {session_id} with improvement score: {session.improvement_score:.3f}\")\n            return summary\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to complete training session: {e}\")\n            raise\n    \n    async def _save_session(self, session: TrainingSession):\n        \"\"\"Save training session to storage\"\"\"\n        try:\n            session_data = {\n                \"session_id\": session.session_id,\n                \"adapter_id\": session.adapter_id,\n                \"legendary_profile\": session.legendary_profile,\n                \"domain\": session.domain,\n                \"phase\": session.phase.value,\n                \"start_time\": session.start_time.isoformat(),\n                \"end_time\": session.end_time.isoformat() if session.end_time else None,\n                \"feedback_count\": session.feedback_count,\n                \"improvement_score\": session.improvement_score,\n                \"training_examples\": session.training_examples,\n                \"feedback_records\": [\n                    {\n                        \"feedback_id\": fb.feedback_id,\n                        \"session_id\": fb.session_id,\n                        \"adapter_id\": fb.adapter_id,\n                        \"prompt\": fb.prompt,\n                        \"original_response\": fb.original_response,\n                        \"feedback_type\": fb.feedback_type.value,\n                        \"feedback_text\": fb.feedback_text,\n                        \"corrected_response\": fb.corrected_response,\n                        \"rating\": fb.rating,\n                        \"user_id\": fb.user_id,\n                        \"timestamp\": fb.timestamp.isoformat(),\n                        \"applied\": fb.applied\n                    } for fb in session.feedback_records\n                ],\n                \"performance_metrics\": session.performance_metrics,\n                \"status\": session.status\n            }\n            \n            session_file = self.training_data_path / \"sessions\" / f\"{session.session_id}.json\"\n            with open(session_file, 'w', encoding='utf-8') as f:\n                json.dump(session_data, f, indent=2)\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to save session: {e}\")\n    \n    async def _save_performance_history(self):\n        \"\"\"Save performance history to storage\"\"\"\n        try:\n            history_file = self.training_data_path / \"performance_history.json\"\n            with open(history_file, 'w', encoding='utf-8') as f:\n                json.dump(self.performance_history, f, indent=2)\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to save performance history: {e}\")\n    \n    async def _save_training_metrics(self):\n        \"\"\"Save training metrics to storage\"\"\"\n        try:\n            metrics_file = self.training_data_path / \"training_metrics.json\"\n            with open(metrics_file, 'w', encoding='utf-8') as f:\n                json.dump(self.training_metrics, f, indent=2)\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to save training metrics: {e}\")\n    \n    async def get_active_sessions(self) -> List[Dict[str, Any]]:\n        \"\"\"Get information about active training sessions\"\"\"\n        sessions = []\n        \n        for session_id, session in self.active_sessions.items():\n            sessions.append({\n                \"session_id\": session_id,\n                \"adapter_id\": session.adapter_id,\n                \"legendary_profile\": session.legendary_profile,\n                \"domain\": session.domain,\n                \"phase\": session.phase.value,\n                \"start_time\": session.start_time.isoformat(),\n                \"feedback_count\": session.feedback_count,\n                \"improvement_score\": session.improvement_score,\n                \"duration_hours\": (datetime.now() - session.start_time).total_seconds() / 3600\n            })\n        \n        return sorted(sessions, key=lambda x: x[\"start_time\"], reverse=True)\n    \n    async def get_training_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get overall training metrics\"\"\"\n        return {\n            **self.training_metrics,\n            \"active_sessions\": len(self.active_sessions),\n            \"pending_feedback\": len(self.feedback_queue),\n            \"adapters_trained\": len(self.performance_history)\n        }\n    \n    async def shutdown(self):\n        \"\"\"Graceful shutdown of Mentor Mode\"\"\"\n        try:\n            # Save all active sessions\n            for session in self.active_sessions.values():\n                await self._save_session(session)\n            \n            # Save metrics and history\n            await self._save_performance_history()\n            await self._save_training_metrics()\n            \n            self.logger.info(\"Mentor Mode shutdown completed\")\n            \n        except Exception as e:\n            self.logger.error(f\"Error during Mentor Mode shutdown: {e}\")